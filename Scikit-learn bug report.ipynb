{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-10-10.0.18362-SP0\n",
      "Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\n",
      "NumPy 1.14.3\n",
      "SciPy 1.1.0\n",
      "Scikit-Learn 0.19.1\n"
     ]
    }
   ],
   "source": [
    "import platform; print(platform.platform())\n",
    "import sys; print(\"Python\", sys.version)\n",
    "import numpy; print(\"NumPy\", numpy.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import sklearn; print(\"Scikit-Learn\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SkLearn# Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
    "\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(corpus):\n",
    "    '''This function return vocab and the idf'''\n",
    "    unique_words = set()#In ths set we will store the word so that we get unique word\n",
    "    for row in corpus:#This for loop will visit each row and split that row and make union with unique_word\n",
    "        unique_words=unique_words.union(row.split())\n",
    "    unique_words=list(unique_words)#Here we are converting set to a list so that we can sort it easly\n",
    "    unique_words.sort()#sorting the list\n",
    "    vocab = {j:i for i,j in enumerate(unique_words)}#Here we are storing word and column in a dictonary\n",
    "\n",
    "    td=len(corpus)#Here we are storing the total no of document in the corpus\n",
    "    td=td+1#we added 1 according to the formula of scikit-learn\n",
    "    b=[]#In this list we will store idf of each word \n",
    "    c=0 #We will keep count in c of the document n which word appear from vocab \n",
    "    for i in list(vocab):#This for loop will itterate each word in vocab for idf\n",
    "        c=1#Here we stated from 1 to according to formula used in scikit-learn\n",
    "        for row in corpus:#This for loop will visit through each document in corpus to check presence of word \n",
    "            if i in row.split():#Here we split the document on space and used the membership function to check presence of word in document\n",
    "                c=c+1#We increse c by 1 when we find the word in document\n",
    "        idf=1+math.log(td/c)#Here we use scikit-learn formula to calculate idf\n",
    "        b.append(idf)#Storing idf in b\n",
    "    df_vocab={i:j for i,j in zip(list(vocab),b)}#Here we are storing word and idf in a dictonary\n",
    "\n",
    "    return vocab,df_vocab#returning vocab and idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,df_vocab=fit(corpus)#Calling fit function on corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab))#Converted dict to list and printed the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.916290731874155, 1.2231435513142097, 1.5108256237659907, 1.0, 1.916290731874155, 1.916290731874155, 1.0, 1.916290731874155, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(list(df_vocab.values()))#Printing the values of dict df_vocab which is idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(corpus,vocab):\n",
    "    '''This function return tfidf'''\n",
    "    rows=[]#Here we will store row no of non zero values\n",
    "    columns=[]#Here we will store column no of non zero values\n",
    "    values=[]#Here we will store non zero values\n",
    "    #All the three rows column and values contain information about same element in diff list\n",
    "    for i,row in enumerate(tqdm(corpus)):#This will go through each document in the corpus\n",
    "        l=len(row.split())#n l we are storing the no of token in documnt\n",
    "        word_freq=dict(Counter(row.split()))#It return count of each token present in document\n",
    "        for word, freq in word_freq.items():#In this for loop we will calculate tfidf and store row ,column and values in respective list\n",
    "            rows.append(i)\n",
    "            columns.append(vocab.get(word))\n",
    "            t=freq/l\n",
    "            t=t*df_vocab.get(word)\n",
    "            values.append(t)\n",
    "    tfidf=csr_matrix((values, (rows,columns)), shape=(len(corpus),len(vocab)))#Here we are covering list rows,columns,values to sparce matrix\n",
    "    tfidf=normalize(tfidf)#Here we used l2 normalization according to scikit-learn\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 4006.02it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf=transform(corpus,vocab)#Calling transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[0])#Printing tfidf of first document in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not matching with scikit-learn result because in scikit-learn there is bug<br>\n",
    "If you change the line 8 of function to l=len(row) then the answer will match with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See result after changingline 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(corpus,vocab):\n",
    "    '''This function return tfidf'''\n",
    "    rows=[]#Here we will store row no of non zero values\n",
    "    columns=[]#Here we will store column no of non zero values\n",
    "    values=[]#Here we will store non zero values\n",
    "    #All the three rows column and values contain information about same element in diff list\n",
    "    for i,row in enumerate(tqdm(corpus)):#This will go through each document in the corpus\n",
    "        l=len(row)#n l we are storing the no of token in documnt\n",
    "        word_freq=dict(Counter(row.split()))#It return count of each token present in document\n",
    "        for word, freq in word_freq.items():#In this for loop we will calculate tfidf and store row ,column and values in respective list\n",
    "            rows.append(i)\n",
    "            columns.append(vocab.get(word))\n",
    "            t=freq/l\n",
    "            t=t*df_vocab.get(word)\n",
    "            values.append(t)\n",
    "    tfidf=csr_matrix((values, (rows,columns)), shape=(len(corpus),len(vocab)))#Here we are covering list rows,columns,values to sparce matrix\n",
    "    tfidf=normalize(tfidf)#Here we used l2 normalization according to scikit-learn\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 4011.77it/s]\n"
     ]
    }
   ],
   "source": [
    "tfidf=transform(corpus,vocab)#Calling transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.46979138557992045\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n"
     ]
    }
   ],
   "source": [
    "print(tfidf[0])#Printing tfidf of first document in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the two result has matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
